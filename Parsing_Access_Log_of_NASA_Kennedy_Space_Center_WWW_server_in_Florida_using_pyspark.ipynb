{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Access Log of NASA Kennedy Space Center WWW server in Florida using pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source\n",
    "\n",
    "\"home/gldata/ NASA_access_log_Aug95.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary\n",
    "\n",
    "Above dataset is access log of NASA Kennedy Space Center WWW server in Florida. The logs are an ASCII file with one line per request, with the following columns:\n",
    "\n",
    "**host** - making the request. A hostname when possible, otherwise the Internet address if the name could not be looked up.\n",
    "\n",
    "**timestamp** - in the format \"DAY MON DD HH:MM:SS YYYY\", where DAY is the day of the week, MON is the name of the month, DD is the day of the month, HH:MM:SS is the time of day using a 24-hour clock, and YYYY is the year. The timezone is -0400.\n",
    "\n",
    "**request url** - Request URL.\n",
    "\n",
    "**HTTP reply code**\n",
    "\n",
    "**bytes returned by the server**\n",
    "\n",
    "Note that from 01/Aug/1995:14:52:01 until 03/Aug/1995:04:36:13 there are no accesses recorded, as the Web server was shut down, due to Hurricane Erin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SparkConf,SQLContext\n",
    "import re\n",
    "from pyspark.sql.types import Row\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparkConf= SparkConf().setAppName(\"NasaLogAssignment\").setMaster(\"local[1]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc= SparkContext(conf=sparkConf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATTERN = '^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+)(.*)\" (\\d{3}) (\\S+)'\n",
    "def parseLogLine(log):\n",
    "    m = re.match(PATTERN, log)\n",
    "    if m:\n",
    "        return [Row(host=m.group(1), timeStamp=m.group(4),url=m.group(6), httpCode=int(m.group(8)))]\n",
    "    else:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logContents = sc.textFile(\"/gldata/NASA_access_log_Aug95.gz\").flatMap(lambda x: parseLogLine(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(host=u'in24.inetnebr.com', httpCode=200, timeStamp=u'01/Aug/1995:00:00:01 -0400', url=u'/shuttle/missions/sts-68/news/sts-68-mcc-05.txt')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logContents.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = [\"host\", \"httpCode\", \"timestamp1\", \"url\"]\n",
    "contentSchema = StructType([\n",
    "    StructField(\"host\",StringType(),True),\n",
    "    StructField(\"httpCode\",IntegerType(),True),\n",
    "    StructField(\"timestamp1\",StringType(),True),\n",
    "    StructField(\"url\",StringType(),True)\n",
    "   ])\n",
    "dfraw = sqlContext.createDataFrame(logContents.map(lambda row: Row(**{x[0]: x[1] for x in zip(columns, row)})), contentSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['host', 'httpCode', 'timestamp1', 'url']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfraw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import concat\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import countDistinct\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split timestamp and discard milliseconds component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_col = split(dfraw['timestamp1'], ' ')\n",
    "dfraw = dfraw.withColumn('datetime', split_col.getItem(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get day, hour, min and seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_col1 = split(dfraw['datetime'],':' )\n",
    "dfraw = dfraw.withColumn('date',split_col1.getItem(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfraw = dfraw.withColumn('hour',split_col1.getItem(1))\n",
    "dfraw = dfraw.withColumn('min',split_col1.getItem(2))\n",
    "dfraw = dfraw.withColumn('second',split_col1.getItem(3))\n",
    "dfraw = dfraw.withColumn('actualTimestamp',concat(col('datetime'),lit(' '),col('hour'),lit(':'),col('min'),lit(':'),col('second')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+----+---+------+\n",
      "|            datetime|       date|     actualTimestamp|hour|min|second|\n",
      "+--------------------+-----------+--------------------+----+---+------+\n",
      "|01/Aug/1995:00:00:01|01/Aug/1995|01/Aug/1995:00:00...|  00| 00|    01|\n",
      "|01/Aug/1995:00:00:07|01/Aug/1995|01/Aug/1995:00:00...|  00| 00|    07|\n",
      "|01/Aug/1995:00:00:08|01/Aug/1995|01/Aug/1995:00:00...|  00| 00|    08|\n",
      "|01/Aug/1995:00:00:08|01/Aug/1995|01/Aug/1995:00:00...|  00| 00|    08|\n",
      "|01/Aug/1995:00:00:08|01/Aug/1995|01/Aug/1995:00:00...|  00| 00|    08|\n",
      "|01/Aug/1995:00:00:09|01/Aug/1995|01/Aug/1995:00:00...|  00| 00|    09|\n",
      "|01/Aug/1995:00:00:10|01/Aug/1995|01/Aug/1995:00:00...|  00| 00|    10|\n",
      "|01/Aug/1995:00:00:10|01/Aug/1995|01/Aug/1995:00:00...|  00| 00|    10|\n",
      "|01/Aug/1995:00:00:10|01/Aug/1995|01/Aug/1995:00:00...|  00| 00|    10|\n",
      "|01/Aug/1995:00:00:11|01/Aug/1995|01/Aug/1995:00:00...|  00| 00|    11|\n",
      "|01/Aug/1995:00:00:12|01/Aug/1995|01/Aug/1995:00:00...|  00| 00|    12|\n",
      "|01/Aug/1995:00:00:12|01/Aug/1995|01/Aug/1995:00:00...|  00| 00|    12|\n",
      "|01/Aug/1995:00:00:13|01/Aug/1995|01/Aug/1995:00:00...|  00| 00|    13|\n",
      "|01/Aug/1995:00:00:14|01/Aug/1995|01/Aug/1995:00:00...|  00| 00|    14|\n",
      "|01/Aug/1995:00:00:16|01/Aug/1995|01/Aug/1995:00:00...|  00| 00|    16|\n",
      "|01/Aug/1995:00:00:17|01/Aug/1995|01/Aug/1995:00:00...|  00| 00|    17|\n",
      "|01/Aug/1995:00:00:18|01/Aug/1995|01/Aug/1995:00:00...|  00| 00|    18|\n",
      "|01/Aug/1995:00:00:19|01/Aug/1995|01/Aug/1995:00:00...|  00| 00|    19|\n",
      "|01/Aug/1995:00:00:19|01/Aug/1995|01/Aug/1995:00:00...|  00| 00|    19|\n",
      "|01/Aug/1995:00:00:20|01/Aug/1995|01/Aug/1995:00:00...|  00| 00|    20|\n",
      "+--------------------+-----------+--------------------+----+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfraw.select('datetime','date','actualTimestamp','hour','min','second').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert date into a date type and then cast the same to a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfraw = dfraw.withColumn('actualdate', to_date('date', 'dd/MMM/yyyy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfraw = dfraw.withColumn('actualdateAsString',dfraw.actualdate.cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+----------+------------------+--------------------+----+---+------+\n",
      "|            datetime|       date|actualdate|actualdateAsString|     actualTimestamp|hour|min|second|\n",
      "+--------------------+-----------+----------+------------------+--------------------+----+---+------+\n",
      "|01/Aug/1995:00:00:01|01/Aug/1995|1995-08-01|        1995-08-01|01/Aug/1995:00:00...|  00| 00|    01|\n",
      "|01/Aug/1995:00:00:07|01/Aug/1995|1995-08-01|        1995-08-01|01/Aug/1995:00:00...|  00| 00|    07|\n",
      "|01/Aug/1995:00:00:08|01/Aug/1995|1995-08-01|        1995-08-01|01/Aug/1995:00:00...|  00| 00|    08|\n",
      "|01/Aug/1995:00:00:08|01/Aug/1995|1995-08-01|        1995-08-01|01/Aug/1995:00:00...|  00| 00|    08|\n",
      "|01/Aug/1995:00:00:08|01/Aug/1995|1995-08-01|        1995-08-01|01/Aug/1995:00:00...|  00| 00|    08|\n",
      "|01/Aug/1995:00:00:09|01/Aug/1995|1995-08-01|        1995-08-01|01/Aug/1995:00:00...|  00| 00|    09|\n",
      "|01/Aug/1995:00:00:10|01/Aug/1995|1995-08-01|        1995-08-01|01/Aug/1995:00:00...|  00| 00|    10|\n",
      "|01/Aug/1995:00:00:10|01/Aug/1995|1995-08-01|        1995-08-01|01/Aug/1995:00:00...|  00| 00|    10|\n",
      "|01/Aug/1995:00:00:10|01/Aug/1995|1995-08-01|        1995-08-01|01/Aug/1995:00:00...|  00| 00|    10|\n",
      "|01/Aug/1995:00:00:11|01/Aug/1995|1995-08-01|        1995-08-01|01/Aug/1995:00:00...|  00| 00|    11|\n",
      "|01/Aug/1995:00:00:12|01/Aug/1995|1995-08-01|        1995-08-01|01/Aug/1995:00:00...|  00| 00|    12|\n",
      "|01/Aug/1995:00:00:12|01/Aug/1995|1995-08-01|        1995-08-01|01/Aug/1995:00:00...|  00| 00|    12|\n",
      "|01/Aug/1995:00:00:13|01/Aug/1995|1995-08-01|        1995-08-01|01/Aug/1995:00:00...|  00| 00|    13|\n",
      "|01/Aug/1995:00:00:14|01/Aug/1995|1995-08-01|        1995-08-01|01/Aug/1995:00:00...|  00| 00|    14|\n",
      "|01/Aug/1995:00:00:16|01/Aug/1995|1995-08-01|        1995-08-01|01/Aug/1995:00:00...|  00| 00|    16|\n",
      "|01/Aug/1995:00:00:17|01/Aug/1995|1995-08-01|        1995-08-01|01/Aug/1995:00:00...|  00| 00|    17|\n",
      "|01/Aug/1995:00:00:18|01/Aug/1995|1995-08-01|        1995-08-01|01/Aug/1995:00:00...|  00| 00|    18|\n",
      "|01/Aug/1995:00:00:19|01/Aug/1995|1995-08-01|        1995-08-01|01/Aug/1995:00:00...|  00| 00|    19|\n",
      "|01/Aug/1995:00:00:19|01/Aug/1995|1995-08-01|        1995-08-01|01/Aug/1995:00:00...|  00| 00|    19|\n",
      "|01/Aug/1995:00:00:20|01/Aug/1995|1995-08-01|        1995-08-01|01/Aug/1995:00:00...|  00| 00|    20|\n",
      "+--------------------+-----------+----------+------------------+--------------------+----+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfraw.select('datetime','date','actualdate','actualdateAsString','actualTimestamp','hour','min','second').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To calculate the day of the week given a date string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weekday(date):\n",
    "    import datetime\n",
    "    import calendar\n",
    "    year, month, day = (int(x) for x in date.split('-'))    \n",
    "    weekday = datetime.date(year, month, day)\n",
    "    return calendar.day_name[weekday.weekday()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "getWeekDay=udf(get_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tuesday'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_weekday('1995-08-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['host',\n",
       " 'httpCode',\n",
       " 'timestamp1',\n",
       " 'url',\n",
       " 'datetime',\n",
       " 'date',\n",
       " 'hour',\n",
       " 'min',\n",
       " 'second',\n",
       " 'actualTimestamp',\n",
       " 'actualdate',\n",
       " 'actualdateAsString']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfraw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfraw = dfraw.withColumn('dayOfWeek',getWeekDay('actualdateAsString'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['host',\n",
       " 'httpCode',\n",
       " 'timestamp1',\n",
       " 'url',\n",
       " 'datetime',\n",
       " 'date',\n",
       " 'hour',\n",
       " 'min',\n",
       " 'second',\n",
       " 'actualTimestamp',\n",
       " 'actualdate',\n",
       " 'actualdateAsString',\n",
       " 'dayOfWeek']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfraw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+----------+------------------+---------+--------------------+----+---+------+\n",
      "|            datetime|       date|actualdate|actualdateAsString|dayOfWeek|     actualTimestamp|hour|min|second|\n",
      "+--------------------+-----------+----------+------------------+---------+--------------------+----+---+------+\n",
      "|01/Aug/1995:00:00:01|01/Aug/1995|1995-08-01|        1995-08-01|  Tuesday|01/Aug/1995:00:00...|  00| 00|    01|\n",
      "|01/Aug/1995:00:00:07|01/Aug/1995|1995-08-01|        1995-08-01|  Tuesday|01/Aug/1995:00:00...|  00| 00|    07|\n",
      "|01/Aug/1995:00:00:08|01/Aug/1995|1995-08-01|        1995-08-01|  Tuesday|01/Aug/1995:00:00...|  00| 00|    08|\n",
      "|01/Aug/1995:00:00:08|01/Aug/1995|1995-08-01|        1995-08-01|  Tuesday|01/Aug/1995:00:00...|  00| 00|    08|\n",
      "|01/Aug/1995:00:00:08|01/Aug/1995|1995-08-01|        1995-08-01|  Tuesday|01/Aug/1995:00:00...|  00| 00|    08|\n",
      "|01/Aug/1995:00:00:09|01/Aug/1995|1995-08-01|        1995-08-01|  Tuesday|01/Aug/1995:00:00...|  00| 00|    09|\n",
      "|01/Aug/1995:00:00:10|01/Aug/1995|1995-08-01|        1995-08-01|  Tuesday|01/Aug/1995:00:00...|  00| 00|    10|\n",
      "|01/Aug/1995:00:00:10|01/Aug/1995|1995-08-01|        1995-08-01|  Tuesday|01/Aug/1995:00:00...|  00| 00|    10|\n",
      "|01/Aug/1995:00:00:10|01/Aug/1995|1995-08-01|        1995-08-01|  Tuesday|01/Aug/1995:00:00...|  00| 00|    10|\n",
      "|01/Aug/1995:00:00:11|01/Aug/1995|1995-08-01|        1995-08-01|  Tuesday|01/Aug/1995:00:00...|  00| 00|    11|\n",
      "|01/Aug/1995:00:00:12|01/Aug/1995|1995-08-01|        1995-08-01|  Tuesday|01/Aug/1995:00:00...|  00| 00|    12|\n",
      "|01/Aug/1995:00:00:12|01/Aug/1995|1995-08-01|        1995-08-01|  Tuesday|01/Aug/1995:00:00...|  00| 00|    12|\n",
      "|01/Aug/1995:00:00:13|01/Aug/1995|1995-08-01|        1995-08-01|  Tuesday|01/Aug/1995:00:00...|  00| 00|    13|\n",
      "|01/Aug/1995:00:00:14|01/Aug/1995|1995-08-01|        1995-08-01|  Tuesday|01/Aug/1995:00:00...|  00| 00|    14|\n",
      "|01/Aug/1995:00:00:16|01/Aug/1995|1995-08-01|        1995-08-01|  Tuesday|01/Aug/1995:00:00...|  00| 00|    16|\n",
      "|01/Aug/1995:00:00:17|01/Aug/1995|1995-08-01|        1995-08-01|  Tuesday|01/Aug/1995:00:00...|  00| 00|    17|\n",
      "|01/Aug/1995:00:00:18|01/Aug/1995|1995-08-01|        1995-08-01|  Tuesday|01/Aug/1995:00:00...|  00| 00|    18|\n",
      "|01/Aug/1995:00:00:19|01/Aug/1995|1995-08-01|        1995-08-01|  Tuesday|01/Aug/1995:00:00...|  00| 00|    19|\n",
      "|01/Aug/1995:00:00:19|01/Aug/1995|1995-08-01|        1995-08-01|  Tuesday|01/Aug/1995:00:00...|  00| 00|    19|\n",
      "|01/Aug/1995:00:00:20|01/Aug/1995|1995-08-01|        1995-08-01|  Tuesday|01/Aug/1995:00:00...|  00| 00|    20|\n",
      "+--------------------+-----------+----------+------------------+---------+--------------------+----+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfraw.select('datetime','date','actualdate','actualdateAsString','dayOfWeek','actualTimestamp','hour','min','second').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Write spark code( using RDD) to find out top 10 requested URLs along with count of number of times they have been requested (This information will help company to find out most popular pages and how frequently they are accessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                 url|count|\n",
      "+--------------------+-----+\n",
      "|/images/NASA-logo...|97410|\n",
      "|/images/KSC-logos...|75337|\n",
      "|/images/MOSAIC-lo...|67448|\n",
      "|/images/USA-logos...|67068|\n",
      "|/images/WORLD-log...|66444|\n",
      "|/images/ksclogo-m...|62778|\n",
      "|           /ksc.html|43687|\n",
      "|/history/apollo/i...|37826|\n",
      "|/images/launch-lo...|35138|\n",
      "|                   /|30347|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfraw.select(\"url\").groupBy(\"url\").count().orderBy('count', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Write spark code to find out top 5 hosts / IP making the request along with count (This information will help company to find out locations where website is popular or to figure out potential DDoS attacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                host|count|\n",
      "+--------------------+-----+\n",
      "|  edams.ksc.nasa.gov| 6530|\n",
      "|piweba4y.prodigy.com| 4846|\n",
      "|        163.206.89.4| 4791|\n",
      "|piweba5y.prodigy.com| 4607|\n",
      "|piweba3y.prodigy.com| 4416|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfraw.select(\"host\").groupBy(\"host\").count().orderBy('count', ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Write spark code to find out top 5 time frame for high traffic (which day of the week or hour of the day receives peak traffic, this information will help company to manage resources for handling peak traffic load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|dayOfWeek| count|\n",
      "+---------+------+\n",
      "| Thursday|304297|\n",
      "|  Tuesday|278749|\n",
      "|Wednesday|255796|\n",
      "|   Friday|234370|\n",
      "|   Monday|228271|\n",
      "+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfraw.select('dayOfWeek').groupBy('dayOfWeek').count().orderBy('count',ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: Write spark code to find out 5 time frames of least traffic (which day of the week or hour of the day receives least traffic, this information will help company to do production deployment in that time frame so that less number of users will be affected if some thing goes wrong during deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-----+\n",
      "|dayOfWeek|hour|count|\n",
      "+---------+----+-----+\n",
      "|   Sunday|  06| 2437|\n",
      "| Saturday|  05| 2579|\n",
      "|   Sunday|  05| 2734|\n",
      "| Saturday|  06| 2748|\n",
      "|   Sunday|  04| 2807|\n",
      "+---------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfraw.select('dayOfWeek', 'hour').groupBy('dayOfWeek','hour').count().orderBy('count').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: Write spark code to find out unique HTTP codes returned by the server along with count (this information is helpful for devops team to find out how many requests are failing so that appropriate action can be taken to fix the issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|httpCode|  count|\n",
      "+--------+-------+\n",
      "|     200|1398988|\n",
      "|     304| 134146|\n",
      "|     302|  26444|\n",
      "|     404|  10056|\n",
      "|     403|    171|\n",
      "|     501|     27|\n",
      "|     500|      3|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfraw.select('httpCode').groupBy('httpCode').count().orderBy('count',ascending=False).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
